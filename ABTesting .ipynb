{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A/B testing is a methodology for testing product changes. You split your users to two groups - the control group which sees the default feature, and an experimental group that sees the new features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, rate is used when you want to measure the usability of the site, and probability when you want to measure the impact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binomial Distribution "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a binomial distribution with probability p , the mean is given by p and the standard deviation is sqrt(p∗(1−p)/N)  where N is the number of trials. A binomial distribution can be used when\n",
    "\n",
    "The outcomes are of 2 types\n",
    "Each event is independent of the other\n",
    "Each event has an identical distribution (i.e. p is the same for all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence Interval "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A confidence interval indicates the range within which the mean is expected to fall in multiple trials of the experiment.\n",
    "\n",
    "For e.g., consider p^ - the proportion of users that click, where N is the number of users. Let us assume a binomial distribution (this requires Np^>5 and N(1−p^)>5). The margin of error is given by\n",
    "m=z∗se and \n",
    "m=z∗sqrt(p^.(1−p^)N)\n",
    "\n",
    "For a 95% confidence interval, z = 1.96."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Testing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The null hypothesis states that the difference between the control and experiment is due to chance. If pcont and ptest are the control and test probabilities, then according to the null hypothesis\n",
    "H0:pexp−pcont=0\n",
    "\n",
    "The alternate hypothesis is that\n",
    "H1:pexp−pcont≠0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing two samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparing two samples, we calculate the pooled standard error. For e.g., suppose Xcont and Ncont are the control number of users that click, and the total number of users in the control group. Let Xexp and Nexp be the values for the experiment. The pooled probability is given by\n",
    "p^pool=Xcont+Xexp/Ncont+Ntest and \n",
    "SEpool=sqrt(p^pool∗(1−p^pool)∗(1Ncont+1Ntest))\n",
    "and d^=p^exp−p^cont\n",
    "and H0:d=0 where d^∼N(0,SEpool)\n",
    "\n",
    "If d^>1.96∗SEpool or d^<−1.96∗SEpool then we can reject the null hypothesis and state that our difference represents a statistically significant difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Significance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Practical significance is the level of change that you would expect to see from a business standpoint for the change to be valuable. What is considered practically significant can vary by field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Size v/s Power Tradeoff "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the decisions is to determine the number of data points needed to get a statistically significant result. This is called statistical power. Power has an inverse trade-off with size. The smaller the change you want to detect or the increased confidence you want to have in the result, means you have to run a larger experiment.\n",
    "\n",
    "As you increase the number of samples, the confidence interval moves closer to the mean\n",
    "\n",
    "α=P(reject null | null true) and \n",
    "β=P(fail to reject null | null false) and \n",
    "1−β is referred to as the sensitivity of the experiment, or statistical power. People often choose high sensitivity, typically around 80%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis of an AB Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "N_cont = 10072  # Control samples (pageviews)\n",
    "N_exp = 9886  # Test samples (pageviews)\n",
    "X_cont = 974  # Control clicks\n",
    "X_exp = 1242  # Exp. clicks\n",
    "d_min=0.02    #minimum practical significance level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_pool=(X_cont+X_exp)/(N_cont+N_exp)\n",
    "SEpool=math.sqrt(p_pool*(1-p_pool)*(1/N_cont+1/N_exp))\n",
    "\n",
    "\n",
    "p_exp=X_exp/N_exp\n",
    "p_cont=X_cont/N_cont\n",
    "d=p_exp - p_cont\n",
    "m=1.96*SEpool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d is 0.028928474040117697and margin of error is 0.008717973932038056\n"
     ]
    }
   ],
   "source": [
    "print(\"d is\"+\" \"+str(d)+\"and margin of error is\"+\" \"+str(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_min=d-m\n",
    "cf_max=d+m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.020210500108079642, 0.03764644797215575)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#confidence interval\n",
    "cf_min,cf_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as minimum of the confidecne level is greater than 0 and minimum practical confidence level, we conclude that click through \n",
    "probability is higher than 0.2 and is significant. Hence, we would launch the experiment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two types of checks\n",
    "\n",
    "Invariant checking: Metrics that shouldn’t change between your test and control\n",
    "Evaluation: High level business metrics, user experience with the product\n",
    "How do we go about making a definition of a metric (for sanity checking)?\n",
    "\n",
    "High level concept of metrics (e.g active users, CTR)\n",
    "Details (e.g. how do you define user activity)\n",
    "Take a set of metrics and summarize them into a single metric (e.g. overall evaluation criterion (OEC))\n",
    "For evaluation, you can choose either one metric or a whole suite of metrics. If you have multiple metric, you can combine them into one metric, such as an objective function, or an Overall Evaluation Criterion (OEC) - a term that Microsoft uses.\n",
    "\n",
    "The last situation is how generally applicable the metric is. If you are running a suite of A/B tests, it is preferable to have a metric that works across the entire suite.\n",
    "\n",
    "User funnel indicates a series of steps taken by users through the site. It is called a funnel because every subsequent stage has fewer users than the stage above. Each stage is a metric - total count, rate, and probability (i.e. a unique user progressed down)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## properities of metrics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Characteristics of a metric:\n",
    "Sensitivity and Robustness: Whether the metric is sensitive to changes you care about, and is robust to changes you don’t care about (e.g. mean is sensitive to outliers, median is robust but not sensitive to changes to small group of users). This can be measured by using prior experiments to see if the metric moves in a way that intuitively make sense. Another alternative is to do A/A tests to see if the metric picks up any spurious differences\n",
    "\n",
    "Distribution: Obtained by doing a distribution on the retrospective data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Absolute vs Relative "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are running a lot of experiments you want to use the relative difference i.e the percentage change. The main advantage of computing the percentage change is that you only have to choose one practical significance boundary to get stability over time. If you are running a lot of experiments over time, your metrics are probably changing over time. Using relative difference helps here by having to use one practical significance boundary rather than change it as the system changes. The main disadvantage is variability, relative differences such as ratios are not as well behaved as absolute differences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Calculating Variability "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to check the variability of a metric to later determine the sizing of the experiment and to analyze confidence intervals and draw conclusions. If we have a metric that varies a lot, then the practical significance level that we are looking for may not be feasible.\n",
    "\n",
    "To calculate the confidence interval, you need\n",
    "\n",
    "1.Variance (or standard deviation) \n",
    "2.Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "import numpy as np\n",
    "vec=[87029, 113407, 84843, 104994, 99327, 92052, 60684]\n",
    "stder=statistics.stdev(vec)/math.sqrt(len(vec))\n",
    "mean=np.mean(vec)\n",
    "conf95_min=mean-1.96*stder\n",
    "conf95_max=mean+1.96*stder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79157.54332794028, 104367.02810063114)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#confidence interval \n",
    "conf95_min,conf95_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Non Parametric Methods "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a way to analyze data without making an assumption about the distribution. At Google, it was observed that the analytical estimates of variance was often under-estimated, and therefore they have resorted to use empirical measurements based on A/A test to evaluate variance. If you see a lot of variability in a metric in an A/A test, it is probably too sensitive to be used. Rather than do several multiple A/A tests, one way is to do a large A/A test, and then do bootstrap to generate small groups and test the variability.\n",
    "\n",
    "With A/A tests, we can\n",
    "\n",
    "1.Compare result to what you expect (sanity check)\n",
    "\n",
    "2.Estimate variance empirically and use your assumption about the distribution to calculate confidence\n",
    "\n",
    "3.Directly estimate confidence interval without making any assumption of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Design Experiment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Designing an Experiment\n",
    "\n",
    "1.Choose subject: What are the units in the population you are going to run the test on? (unit of diversion)\n",
    "\n",
    "2.Choose population: What population are you going to use (US only?)\n",
    "\n",
    "3.Size\n",
    "\n",
    "4.Duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Unit of Diversion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commonly used units of diversion are:\n",
    "\n",
    "User identifier (id): Typically the username or email address used on the website. It is typically stable and unchanging. If user id is used as a unit of diversion, then it is either in the test group or the control group. User ID is personally identifiable\n",
    "\n",
    "Anonymous id: This is usually an anonymous identifier such as a cookie. It changes with browser or device. People may often refresh their cookies every time they log in. It is difficult to refresh a cookie on an app or a phone compared to the computer.\n",
    "\n",
    "Event: An event is a page load that can change for each user. This is used typically for changes that is not user facing.\n",
    "\n",
    "Lesser used units of diversion are\n",
    "\n",
    "Device id: Typically available for mobile devices. It is tied to a specific device and cannot be changed by the user.\n",
    "\n",
    "IP address: The ip address is location specific, but may change as the user changes location (e.g. testing on infrastructure change to test impact on latency)\n",
    "\n",
    "Variability is higher when it is calculated empirically than when calculated analytically. This is because the unit of analysis (i.e. the denominator in the metric) is different from the unit of variability.\n",
    "\n",
    "E.g. If unit of diversion is a query, then coverage (= #queries with ads/ # queries) will have lower variability compared to using a cookie as a unit of diversion. This is because when a query is used, the unit of diversion matches the unit of analysis (which is the denominator of the metric i.e. query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_cont = 50000 + 6021\n",
    "X_cont = 2500 + 302\n",
    "N_exp = 50000 + 5979\n",
    "X_exp = 2500 + 374"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_pool=(X_cont+X_exp)/(N_cont+N_exp)\n",
    "SEpool=math.sqrt(p_pool*(1-p_pool)*(1/N_cont+1/N_exp))\n",
    "\n",
    "\n",
    "p_exp=X_exp/N_exp\n",
    "p_cont=X_cont/N_cont\n",
    "d=p_exp - p_cont\n",
    "m=1.96*SEpool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d is 0.0013237234004343165and margin of error is 0.0025691881506085417\n"
     ]
    }
   ],
   "source": [
    "print(\"d is\"+\" \"+str(d)+\"and margin of error is\"+\" \"+str(m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As d which is p_exp-p_cont is less than the margin of error, the global difference is not statistically significant "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Analyzing Results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Checks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the first things to do once you finish collecting experimental data is to analyze the invariants. This is done by calculating the values for one or more invariants on the test and control group, and check if the difference is statistically significant. For e.g. if the values for an invariant (say total # of cookies) are x and y, then calculate the se as sqrt(0.5∗0.5/x+y), since one would expect the same number of cookies in both groups. Then calculate the margin as 1.96∗se. If the margin is greater than x/(x+y)−y/(x+y), then the difference of the invariant is insignificant. However if the difference is greater than the margin, then the difference is insignifiant and needs to be investigated further\n",
    "\n",
    "An example is provided below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_event=[2451,2475,2394,2482,2374,1704,1468]\n",
    "test_event=[2404,2507,2376,2444,2504,1612,1465]\n",
    "sum_control=sum(control_event)\n",
    "sum_test=sum(test_event)\n",
    "se=math.sqrt((0.5*0.5)/(sum_control+sum_test))\n",
    "margin=1.96*se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.0011741682974559797 is less than the margin which is 0.005596802740247507'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(sum_control/(sum_control+sum_test) - sum_test/(sum_control+sum_test)) + \" \" + \"is less than the margin which is \" + str(margin) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Analysis with a single metric "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_cont = [196, 200, 200, 216, 212, 185, 225, 187, 205, 211, 192, 196, 223, 192]\n",
    "X_cont = [2029, 1991, 1951, 1985, 1973, 2021, 2041, 1980, 1951, 1988, 1977, 2019, 2035, 2007]\n",
    "N_exp = [179, 208, 205, 175, 191, 291, 278, 216, 225, 207, 205, 200, 297, 299]\n",
    "X_exp = [1971, 2009, 2049, 2015, 2027, 1979, 1959, 2020, 2049, 2012, 2023, 1981, 1965, 1993]\n",
    "\n",
    "N_cont_sum = sum(N_cont)\n",
    "X_cont_sum = sum(X_cont)\n",
    "N_exp_sum = sum(N_exp)\n",
    "X_exp_sum = sum(X_exp)\n",
    "\n",
    "p_cont=X_cont_sum/N_cont_sum\n",
    "p_exp=X_exp_sum/N_exp_sum\n",
    "\n",
    "# Empirical standard error and count provided\n",
    "empirical_se = 0.0062\n",
    "empirical_ct = 5000\n",
    "se = (math.sqrt(1/N_cont_sum + 1/N_exp_sum))*empirical_se/math.sqrt(1/empirical_ct + 1/empirical_ct)\n",
    "\n",
    "d=p_cont - p_exp\n",
    "margin=se*1.96\n",
    "\n",
    "d_c95min = d - margin\n",
    "d_c95max = d + margin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d is 1.0083513676517537and margin of error is 0.0156917818580486\n"
     ]
    }
   ],
   "source": [
    "print(\"d is\"+\" \"+str(d)+\"and margin of error is\"+\" \"+str(margin))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udacity.com/course/ab-testing--ud257\n",
    "\n",
    "https://towardsdatascience.com/a-summary-of-udacity-a-b-testing-course-9ecc32dedbb1\n",
    "\n",
    "https://rpubs.com/superseer/ab_testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
